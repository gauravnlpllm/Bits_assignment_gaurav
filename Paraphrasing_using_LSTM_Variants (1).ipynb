{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_rorsqlcCtS"
   },
   "source": [
    "# Paraphrasing using LSTM, Bidirectional LSTM, Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnwPjHZTb5Za"
   },
   "source": [
    "We will be implementing paraphrasing task in this notebook, you will learn how to create a language model for paraphrasing of natural language text by implement and training LSTM.\n",
    "\n",
    "Generating News headlines\n",
    "In this kernel, I will be using the dataset of Google’s PAWS: Paraphrase Adversaries from Word Scrambling: to train a paraphrasing language model.\n",
    "It focuses on generating challenging sentence pairs by using paraphrasing techniques.\n",
    "\n",
    "\n",
    "1. Import the libraries\n",
    "As the first step, we need to import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_-ef8jC8VIcK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vSSarKKuc7TF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Fd1uejQcNRa"
   },
   "source": [
    "## 2. Load the dataset\n",
    "\n",
    "Load the dataset where use second coulmn as original sentence and third coulmn as paraphrase sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sSksJ0XMchbe"
   },
   "outputs": [],
   "source": [
    "# Read TSV file\n",
    "#curr_dir = '/content/'\n",
    "filename = 'train.tsv'  # Replace with your TSV file name\n",
    "data_df = pd.read_csv(filename, delimiter='\\t')\n",
    "\n",
    "# Filter out rows with missing values\n",
    "data_df = data_df.dropna()\n",
    "\n",
    "# Create sentence pairs from 'sentence1' and 'sentence2' columns\n",
    "sentences = data_df['sentence1'].tolist()\n",
    "paraphrases = data_df['sentence2'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAPoLa9AcYCK"
   },
   "source": [
    "### 3 Generating Sequence of N-gram Tokens\n",
    "\n",
    "IT requires a sequence input data, as given a sequence (of words/tokens) the aim to rephrase word/token.  \n",
    "\n",
    "The next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Python’s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zAzWVDXdbW6B"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences + paraphrases)\n",
    "\n",
    "# Convert sentences to sequences\n",
    "sentences_seq = tokenizer.texts_to_sequences(sentences)\n",
    "paraphrases_seq = tokenizer.texts_to_sequences(paraphrases)\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(len(seq) for seq in sentences_seq + paraphrases_seq)\n",
    "sentences_seq = pad_sequences(sentences_seq, maxlen=max_length, padding='post')\n",
    "paraphrases_seq = pad_sequences(paraphrases_seq, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeTMg0s9h2Vb",
    "outputId": "0cc5861a-e44d-4c16-fee7-58e01e787b3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,    63,     7, 10464,     2,  4980,    11,     1,   249,\n",
       "          116,     2,  1446,     9,  4229,  5981, 14252, 14253,     3,\n",
       "          199,   468, 14254,  2801,   558,  7641,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0],\n",
       "       [    1, 10873,  4573,  6706,    28,  1309,     1,   191,   238,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_seq[:2]  ## to visualise the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQhF-tiodNeM"
   },
   "source": [
    "\n",
    "\n",
    "## 4. LSTMs for paraphrasing\n",
    "\n",
    "Unlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n",
    "\n",
    "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n",
    "\n",
    "LSTMs have an additional state called ‘cell state’ through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code. I have added total three layers in the model.\n",
    "\n",
    "1. Embedding Layer : Takes the sequence of words as input\n",
    "2. LSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\n",
    "4. Dense/Output Layer : Computes the probability of the best possible next word as output\n",
    "\n",
    "We will run this model for total 10 epochs but it can be experimented further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNc7ORTmdrMc"
   },
   "source": [
    "LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nHuh79sASaJS"
   },
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index) + 1, 50, input_length=max_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(max_length, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tTla8KUdt3e"
   },
   "source": [
    "Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fu9rqcBiSfJz"
   },
   "outputs": [],
   "source": [
    "def create_bidirectional_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index) + 1, 50, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(Dense(max_length, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vO83uoHydxpS"
   },
   "source": [
    "Stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uuXQWXDbShpB"
   },
   "outputs": [],
   "source": [
    "def create_stacked_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(tokenizer.word_index) + 1, 50, input_length=max_length))\n",
    "    model.add(LSTM(50, return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(max_length, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jir6AJ3wi5lk"
   },
   "source": [
    "you can run the summary function to differentiate the lstm model variants layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYgtm_FUUL7Z",
    "outputId": "16997b25-7cdd-4a4b-fa23-1a68d136b341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_9 (Embedding)     (None, 33, 50)            723450    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 100)               60400     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 33)                3333      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787183 (3.00 MB)\n",
      "Trainable params: 787183 (3.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxKdcDWbdavD",
    "outputId": "fc113426-b4d5-48a9-8214-866e8784fac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 33, 50)            723450    \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 200)               120800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 850883 (3.25 MB)\n",
      "Trainable params: 850883 (3.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bidirectional_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8SSHV2JdfKr",
    "outputId": "616e098f-8e4f-4781-f982-f68e185b1287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, 33, 50)            723450    \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 33, 50)            20200     \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 33)                1683      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 765533 (2.92 MB)\n",
      "Trainable params: 765533 (2.92 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stacked_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL4RDIkFdXPG"
   },
   "source": [
    "Lets train our model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OB4efTYSjcx",
    "outputId": "9c3803cb-48cc-4071-bea8-11a7acfd5872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x795e004589a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = create_lstm_model()\n",
    "lstm_model.fit(sentences_seq, paraphrases_seq, epochs=10, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fNSmef8lSpkB",
    "outputId": "202ad488-d2fa-4dd9-b817-058365ad1477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x795d9c187610>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bidirectional_lstm_model = create_bidirectional_lstm_model()\n",
    "bidirectional_lstm_model.fit(sentences_seq, paraphrases_seq, epochs=10, verbose=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NXROTS0Sskh",
    "outputId": "4ecefd51-b6d7-4572-d3c8-b010bc44eb52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 2/10\n",
      "Epoch 3/10\n",
      "Epoch 4/10\n",
      "Epoch 5/10\n",
      "Epoch 6/10\n",
      "Epoch 7/10\n",
      "Epoch 8/10\n",
      "Epoch 9/10\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x795d2c59f370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_lstm_model = create_stacked_lstm_model()\n",
    "stacked_lstm_model.fit(sentences_seq, paraphrases_seq, epochs=10, verbose=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eau82swTd2Q2"
   },
   "source": [
    "Great, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Correct the padding in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfIOUZfUehmT",
    "outputId": "bca325df-6198-4422-c074-78498fe9cc9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LSTM model:\n",
      "1/1 [==============================] - 0s 411ms/step\n",
      "Original: I enjoy coding a lot -> Paraphrase: the\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Original: Programming gives me joy -> Paraphrase: the\n",
      "\n",
      "Testing Bidirectional LSTM model:\n",
      "1/1 [==============================] - 1s 671ms/step\n",
      "Original: I enjoy coding a lot -> Paraphrase: the\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Original: Programming gives me joy -> Paraphrase: the\n",
      "\n",
      "Testing Stacked LSTM model:\n",
      "1/1 [==============================] - 1s 689ms/step\n",
      "Original: I enjoy coding a lot -> Paraphrase: the\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Original: Programming gives me joy -> Paraphrase: the\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, sentence):\n",
    "    # Convert the input sentence to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([sentence])\n",
    "\n",
    "    # Pad the sequence\n",
    "    sequence = pad_sequences(sequence, maxlen=33, padding='post')\n",
    "\n",
    "    # Predict paraphrased sequence\n",
    "    predicted_seq = model.predict(sequence)\n",
    "\n",
    "    # Convert predicted sequence back to text\n",
    "    predicted_sentence = []\n",
    "    for idx in np.argmax(predicted_seq, axis=1):\n",
    "        word = tokenizer.index_word.get(idx, '')\n",
    "        if word:\n",
    "            predicted_sentence.append(word)\n",
    "        if word == '<end>':  # Assuming '<end>' is the end token\n",
    "            break\n",
    "\n",
    "    predicted_sentence = ' '.join(predicted_sentence)\n",
    "\n",
    "    return predicted_sentence\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"I enjoy coding a lot\",\n",
    "    \"Programming gives me joy\",\n",
    "]\n",
    "\n",
    "# Test each model\n",
    "print(\"Testing LSTM model:\")\n",
    "for sentence in test_sentences:\n",
    "    paraphrase = test_model(lstm_model, sentence)\n",
    "    print(f\"Original: {sentence} -> Paraphrase: {paraphrase}\")\n",
    "\n",
    "print(\"\\nTesting Bidirectional LSTM model:\")\n",
    "for sentence in test_sentences:\n",
    "    paraphrase = test_model(bidirectional_lstm_model, sentence)\n",
    "    print(f\"Original: {sentence} -> Paraphrase: {paraphrase}\")\n",
    "\n",
    "print(\"\\nTesting Stacked LSTM model:\")\n",
    "for sentence in test_sentences:\n",
    "    paraphrase = test_model(stacked_lstm_model, sentence)\n",
    "    print(f\"Original: {sentence} -> Paraphrase: {paraphrase}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPiajLW4eUBT"
   },
   "source": [
    "This code will test each model with the provided test sentences and print out the original sentence along with its paraphrased version according to each model.\n",
    "\n",
    "Run this testing code after training the models to see the paraphrased results for the test sentences. Remember, the quality of paraphrasing might low or vary based on the complexity of the sentence and the model's training data. Adjustments like adding more data or tuning hyperparameters could improve the results. This is a example to learn the implementation of LSTM varients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VvU6aSXEfIYC"
   },
   "source": [
    "### **EXERCISE:-** You many change the hypermarameters of the model and see the difference in results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
