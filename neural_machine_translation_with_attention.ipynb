{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adTDe2CTh3MU"
      },
      "source": [
        "## Translating Human Readable Dates Into Machine Readable Dates\n",
        "\n",
        "* We will build a Neural Machine Translation (NMT) model to translate human-readable dates (\"25th of June, 2009\") into machine-readable dates (\"2009-06-25\") usin biredctional LSTM model with attention.\n",
        "\n",
        "* We will perform \"date translation\" task.\n",
        "* The network will input a date written in a variety of possible formats and the network learn to output dates in the common machine-readable format YYYY-MM-DD.\n",
        "\n",
        "<!--\n",
        "Take a look at [nmt_utils.py](./nmt_utils.py) to see all the formatting. Count and figure out how the formats work, you will need this knowledge later. !-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH6LKsQS72ka"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Import Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RcBRMzPiiMmp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729cad02-a45a-4e0a-a9ef-14c1ab71c5ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Faker==2.0.5\n",
            "  Downloading Faker-2.0.5-py2.py3-none-any.whl (966 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m966.4/966.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker==2.0.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from Faker==2.0.5) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.10/dist-packages (from Faker==2.0.5) (1.3)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-2.0.5\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "!pip install Faker==2.0.5\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BhEaJvph3Mf"
      },
      "source": [
        "<a name='1-1'></a>\n",
        "### 1.1 - Dataset Preprocessing\n",
        "\n",
        "We will train the model on a dataset of 15,000 human readable dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwIf5l17h3Mg",
        "outputId": "8fd8f096-50e7-4c23-fde8-02df4b01cf43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15000/15000 [00:00<00:00, 22321.09it/s]\n"
          ]
        }
      ],
      "source": [
        "m = 15000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCTqMyPch3Mg",
        "outputId": "9149ea3e-8107-480b-b283-02865d5233b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('9 may 1998', '1998-05-09')]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset[:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao4Ffrkxh3Mg"
      },
      "source": [
        "\n",
        "Let's preprocess the data and map the raw text data into the index values.\n",
        "- We will set Tx=30\n",
        "    - We assume Tx is the maximum length of the human readable date.\n",
        "    - If we get a longer input, we would have to truncate it.\n",
        "- We will set Ty=10\n",
        "    - \"YYYY-MM-DD\" is 10 characters long."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdso90EBh3Mg",
        "outputId": "70bfc596-4673-4235-d8e1-ac7430f7b180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (15000, 30)\n",
            "Y.shape: (15000, 10)\n",
            "Xoh.shape: (15000, 30, 37)\n",
            "Yoh.shape: (15000, 10, 11)\n"
          ]
        }
      ],
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "#`dataset`: a list of tuples of (human readable date, machine readable date).\n",
        "#`human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index.\n",
        "#`machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index.\n",
        "\n",
        "print(\"X.shape:\", X.shape)  # Each date is padded to ensure a length of $T_x$ using a special character (< pad >).`X.shape = (m, Tx)` where m is the number of training examples in a batch.\n",
        "print(\"Y.shape:\", Y.shape)  # Each character is replaced by the index (integer) it is mapped to in machine_vocab. Y.shape = (m, Ty)\n",
        "print(\"Xoh.shape:\", Xoh.shape)  #one-hot version of X.... Xoh.shape = (m, Tx, len(human_vocab))\n",
        "print(\"Yoh.shape:\", Yoh.shape)  #Each index in Y is converted to the one-hot representation. Yoh.shape = (m, Ty, len(machine_vocab)). len(machine_vocab) = 11 since there are 10 numeric digits (0 to 9) and the - symbol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUOayR4gh3Mh",
        "outputId": "8c49d696-1251-45b0-fcff-89dc843451fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source date: 10.11.19\n",
            "Target date: 2019-11-10\n",
            "\n",
            "Source after preprocessing (indices): [ 4  3  1  4  4  1  4 12 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [ 3  1  2 10  0  2  2  0  2  1]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "index = 1\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94o4RYbOh3Mi"
      },
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Neural Machine Translation with Attention\n",
        "\"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
        "The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2TkQnykh3Mi"
      },
      "source": [
        "\n",
        "\n",
        "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
        "\n",
        "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
        "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
        "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
        "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ukmqe_Yh3Ml"
      },
      "source": [
        "#### Implementation Details\n",
        "   \n",
        "Let's implement this neural translator. You will start by implementing two functions: `one_step_attention()` and `model()`.\n",
        "\n",
        "#### one_step_attention\n",
        "\n",
        "    \n",
        "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cvop5Apyh3Mm"
      },
      "outputs": [],
      "source": [
        "# Defined shared layers as global variables\n",
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mZuMOnTDh3Mn"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: one_step_attention\n",
        "\n",
        "def one_step_attention(a, s_prev):\n",
        "    \"\"\"\n",
        "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
        "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
        "\n",
        "    Arguments:\n",
        "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
        "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
        "\n",
        "    Returns:\n",
        "    context -- context vector, input of the next (post-attention) LSTM cell\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
        "    s_prev = repeator(s_prev)\n",
        "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
        "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
        "    concat = concatenator([a, s_prev])\n",
        "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
        "    e = densor1(concat)\n",
        "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
        "    energies = densor2(e)\n",
        "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
        "    alphas = activator(energies)\n",
        "    # Use dotor together with \"alphas\" and \"a\", in this order, to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
        "    context = dotor([alphas, a])\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "47Pz4zwe72ke",
        "outputId": "2e1a6b2b-a13c-4ceb-bca7-cea3968f9e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ],
      "source": [
        "# UNIT TEST\n",
        "from tensorflow import python as tf_python\n",
        "def one_step_attention_test(target):\n",
        "\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    #np.random.seed(10)\n",
        "    a = np.random.uniform(1, 0, (m, Tx, 2 * n_a)).astype(np.float32)\n",
        "    s_prev =np.random.uniform(1, 0, (m, n_s)).astype(np.float32) * 1\n",
        "    context = target(a, s_prev)\n",
        "\n",
        "    assert type(context) == tf_python.framework.ops.EagerTensor, \"Unexpected type. It should be a Tensor\"\n",
        "    assert tuple(context.shape) == (m, 1, n_s), \"Unexpected output shape\"\n",
        "    assert np.all(context.numpy() > 0), \"All output values must be > 0 in this example\"\n",
        "    assert np.all(context.numpy() < 1), \"All output values must be < 1 in this example\"\n",
        "\n",
        "    #assert np.allclose(context[0][0][0:5].numpy(), [0.50877404, 0.57160693, 0.45448175, 0.50074816, 0.53651875]), \"Unexpected values in the result\"\n",
        "    print(\"\\033[92mAll tests passed!\")\n",
        "\n",
        "one_step_attention_test(one_step_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcmC3WcQh3Mn"
      },
      "source": [
        "<a name='ex-2'></a>\n",
        "### Exercise 2 - modelf\n",
        "\n",
        "Implement `modelf()` as explained in figure 1 and the instructions:\n",
        "\n",
        "* `modelf` first runs the input through a Bi-LSTM to get $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$.\n",
        "* Then, `modelf` calls `one_step_attention()` $T_y$ times using a `for` loop.  At each iteration of this loop:\n",
        "    - It gives the computed context vector $context^{<t>}$ to the post-attention LSTM.\n",
        "    - It runs the output of the post-attention LSTM through a dense layer with softmax activation.\n",
        "    - The softmax generates a prediction $\\hat{y}^{<t>}$.\n",
        "    \n",
        "Again, we have defined global layers that will share weights to be used in `modelf()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5RHgmZrVh3Mo"
      },
      "outputs": [],
      "source": [
        "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
        "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
        "\n",
        "# Please note, this is the post attention LSTM cell.\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qeKbeDOvh3Mo"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: model\n",
        "\n",
        "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Tx -- length of the input sequence\n",
        "    Ty -- length of the output sequence\n",
        "    n_a -- hidden state size of the Bi-LSTM\n",
        "    n_s -- hidden state size of the post-attention LSTM\n",
        "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
        "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
        "\n",
        "    Returns:\n",
        "    model -- Keras model instance\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the inputs of your model with a shape (Tx,)\n",
        "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
        "    # for the decoder LSTM with shape (n_s,)\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "\n",
        "    # Initialize empty list of outputs\n",
        "    outputs = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n",
        "\n",
        "    # Step 2: Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "\n",
        "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
        "        context = one_step_attention(a, s)\n",
        "\n",
        "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state = [s, c])\n",
        "\n",
        "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
        "        out = output_layer(s)\n",
        "\n",
        "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
        "        outputs.append(out)\n",
        "\n",
        "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
        "    model = Model([X, s0, c0], outputs)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IOUhIkjq72kg"
      },
      "outputs": [],
      "source": [
        "\n",
        "def modelf_test(target):\n",
        "    m = 10\n",
        "    Tx = 30\n",
        "    n_a = 32\n",
        "    n_s = 64\n",
        "    len_human_vocab = 37\n",
        "    len_machine_vocab = 11\n",
        "\n",
        "\n",
        "    model = target(Tx, Ty, n_a, n_s, len_human_vocab, len_machine_vocab)\n",
        "\n",
        "\n",
        "    expected_summary = [['InputLayer', [(None, 30, 37)], 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['Bidirectional', (None, 30, 64), 17920],\n",
        "                         ['RepeatVector', (None, 30, 64), 0, 30],\n",
        "                         ['Concatenate', (None, 30, 128), 0],\n",
        "                         ['Dense', (None, 30, 10), 1290, 'tanh'],\n",
        "                         ['Dense', (None, 30, 1), 11, 'relu'],\n",
        "                         ['Activation', (None, 30, 1), 0],\n",
        "                         ['Dot', (None, 1, 64), 0],\n",
        "                         ['InputLayer', [(None, 64)], 0],\n",
        "                         ['LSTM',[(None, 64), (None, 64), (None, 64)], 33024,[(None, 1, 64), (None, 64), (None, 64)],'tanh'],\n",
        "                         ['Dense', (None, 11), 715, 'softmax']]\n",
        "\n",
        "    assert len(model.outputs) == 10, f\"Wrong output shape. Expected 10 != {len(model.outputs)}\"\n",
        "\n",
        "\n",
        "modelf_test(modelf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--RX7hSsh3Mo"
      },
      "source": [
        "Run the following cell to create your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "psdd-Ac6h3Mp"
      },
      "outputs": [],
      "source": [
        "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgeU_I9_h3Mp"
      },
      "source": [
        "Let's get a summary of the model to check if it matches the expected output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX0vaYmPh3Mq",
        "outputId": "3832e5df-8a84-4d42-a753-2b203994580b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 30, 37)]             0         []                            \n",
            "                                                                                                  \n",
            " s0 (InputLayer)             [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  (None, 30, 64)               17920     ['input_2[0][0]']             \n",
            " onal)                                                                                            \n",
            "                                                                                                  \n",
            " repeat_vector (RepeatVecto  (None, 30, 64)               0         ['s0[0][0]',                  \n",
            " r)                                                                  'lstm[10][0]',               \n",
            "                                                                     'lstm[11][0]',               \n",
            "                                                                     'lstm[12][0]',               \n",
            "                                                                     'lstm[13][0]',               \n",
            "                                                                     'lstm[14][0]',               \n",
            "                                                                     'lstm[15][0]',               \n",
            "                                                                     'lstm[16][0]',               \n",
            "                                                                     'lstm[17][0]',               \n",
            "                                                                     'lstm[18][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 30, 128)              0         ['bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[10][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[11][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[12][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[13][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[14][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[15][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[16][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[17][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[18][0]',      \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'repeat_vector[19][0]']      \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 30, 10)               1290      ['concatenate[10][0]',        \n",
            "                                                                     'concatenate[11][0]',        \n",
            "                                                                     'concatenate[12][0]',        \n",
            "                                                                     'concatenate[13][0]',        \n",
            "                                                                     'concatenate[14][0]',        \n",
            "                                                                     'concatenate[15][0]',        \n",
            "                                                                     'concatenate[16][0]',        \n",
            "                                                                     'concatenate[17][0]',        \n",
            "                                                                     'concatenate[18][0]',        \n",
            "                                                                     'concatenate[19][0]']        \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 30, 1)                11        ['dense[10][0]',              \n",
            "                                                                     'dense[11][0]',              \n",
            "                                                                     'dense[12][0]',              \n",
            "                                                                     'dense[13][0]',              \n",
            "                                                                     'dense[14][0]',              \n",
            "                                                                     'dense[15][0]',              \n",
            "                                                                     'dense[16][0]',              \n",
            "                                                                     'dense[17][0]',              \n",
            "                                                                     'dense[18][0]',              \n",
            "                                                                     'dense[19][0]']              \n",
            "                                                                                                  \n",
            " attention_weights (Activat  (None, 30, 1)                0         ['dense_1[10][0]',            \n",
            " ion)                                                                'dense_1[11][0]',            \n",
            "                                                                     'dense_1[12][0]',            \n",
            "                                                                     'dense_1[13][0]',            \n",
            "                                                                     'dense_1[14][0]',            \n",
            "                                                                     'dense_1[15][0]',            \n",
            "                                                                     'dense_1[16][0]',            \n",
            "                                                                     'dense_1[17][0]',            \n",
            "                                                                     'dense_1[18][0]',            \n",
            "                                                                     'dense_1[19][0]']            \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1, 64)                0         ['attention_weights[10][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[11][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[12][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[13][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[14][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[15][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[16][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[17][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[18][0]',  \n",
            "                                                                     'bidirectional_1[0][0]',     \n",
            "                                                                     'attention_weights[19][0]',  \n",
            "                                                                     'bidirectional_1[0][0]']     \n",
            "                                                                                                  \n",
            " c0 (InputLayer)             [(None, 64)]                 0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 64),                 33024     ['dot[10][0]',                \n",
            "                              (None, 64),                            's0[0][0]',                  \n",
            "                              (None, 64)]                            'c0[0][0]',                  \n",
            "                                                                     'dot[11][0]',                \n",
            "                                                                     'lstm[10][0]',               \n",
            "                                                                     'lstm[10][2]',               \n",
            "                                                                     'dot[12][0]',                \n",
            "                                                                     'lstm[11][0]',               \n",
            "                                                                     'lstm[11][2]',               \n",
            "                                                                     'dot[13][0]',                \n",
            "                                                                     'lstm[12][0]',               \n",
            "                                                                     'lstm[12][2]',               \n",
            "                                                                     'dot[14][0]',                \n",
            "                                                                     'lstm[13][0]',               \n",
            "                                                                     'lstm[13][2]',               \n",
            "                                                                     'dot[15][0]',                \n",
            "                                                                     'lstm[14][0]',               \n",
            "                                                                     'lstm[14][2]',               \n",
            "                                                                     'dot[16][0]',                \n",
            "                                                                     'lstm[15][0]',               \n",
            "                                                                     'lstm[15][2]',               \n",
            "                                                                     'dot[17][0]',                \n",
            "                                                                     'lstm[16][0]',               \n",
            "                                                                     'lstm[16][2]',               \n",
            "                                                                     'dot[18][0]',                \n",
            "                                                                     'lstm[17][0]',               \n",
            "                                                                     'lstm[17][2]',               \n",
            "                                                                     'dot[19][0]',                \n",
            "                                                                     'lstm[18][0]',               \n",
            "                                                                     'lstm[18][2]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 11)                   715       ['lstm[10][0]',               \n",
            "                                                                     'lstm[11][0]',               \n",
            "                                                                     'lstm[12][0]',               \n",
            "                                                                     'lstm[13][0]',               \n",
            "                                                                     'lstm[14][0]',               \n",
            "                                                                     'lstm[15][0]',               \n",
            "                                                                     'lstm[16][0]',               \n",
            "                                                                     'lstm[17][0]',               \n",
            "                                                                     'lstm[18][0]',               \n",
            "                                                                     'lstm[19][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 52960 (206.88 KB)\n",
            "Trainable params: 52960 (206.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u3D9Odhh3Ms"
      },
      "source": [
        "<a name='ex-3'></a>\n",
        "### Exercise 3 - Compile the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sBFRJ49rh3Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "612e6c90-aafb-4e4e-9282-79bb76d76501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "### START CODE HERE ### (≈2 lines)\n",
        "opt = tf.keras.optimizers.legacy.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gKo6AL5472ki",
        "outputId": "d2036d6b-607e-4783-e02e-09915ed99b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mAll tests passed!\n"
          ]
        }
      ],
      "source": [
        "# UNIT TESTS\n",
        "assert opt.lr == 0.005, \"Set the lr parameter to 0.005\"\n",
        "assert opt.beta_1 == 0.9, \"Set the beta_1 parameter to 0.9\"\n",
        "assert opt.beta_2 == 0.999, \"Set the beta_2 parameter to 0.999\"\n",
        "assert opt.decay == 0.01, \"Set the decay parameter to 0.01\"\n",
        "assert model.loss == \"categorical_crossentropy\", \"Wrong loss. Use 'categorical_crossentropy'\"\n",
        "assert model.optimizer == opt, \"Use the optimizer that you have instantiated\"\n",
        "assert model.compiled_metrics._user_metrics[0] == 'accuracy', \"set metrics to ['accuracy']\"\n",
        "\n",
        "print(\"\\033[92mAll tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz71nM3oh3Ms"
      },
      "source": [
        "#### Define inputs and outputs, and fit the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "USFiNKYhh3Mt"
      },
      "outputs": [],
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVkITGi3h3Mt"
      },
      "source": [
        "Let's now fit the model and run it for one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPuwY45bh3Mt",
        "outputId": "ccf165ec-bcf3-469f-d02a-96c0a0b6b4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150/150 [==============================] - 31s 81ms/step - loss: 14.3473 - dense_2_loss: 0.8693 - dense_2_1_loss: 0.7603 - dense_2_2_loss: 1.6001 - dense_2_3_loss: 2.5223 - dense_2_4_loss: 0.4838 - dense_2_5_loss: 1.0341 - dense_2_6_loss: 2.4721 - dense_2_7_loss: 0.5877 - dense_2_8_loss: 1.5364 - dense_2_9_loss: 2.4810 - dense_2_accuracy: 0.6651 - dense_2_1_accuracy: 0.7769 - dense_2_2_accuracy: 0.3421 - dense_2_3_accuracy: 0.1003 - dense_2_4_accuracy: 0.9673 - dense_2_5_accuracy: 0.5045 - dense_2_6_accuracy: 0.1117 - dense_2_7_accuracy: 0.9633 - dense_2_8_accuracy: 0.2777 - dense_2_9_accuracy: 0.1006\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7833919143d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUUD9yXxh3Mu"
      },
      "source": [
        "You can now see the results on new examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ8sd_cuh3Mv",
        "outputId": "cb5a430d-a3fe-4aec-d938-a34dccd05d5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 8s 8s/step\n",
            "source: 3 May 1979\n",
            "output: 1994-05-17 \n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "source: 5 April 09\n",
            "output: 2015-05-15 \n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-dfa82594874d>:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  output = [inv_machine_vocab[int(i)] for i in prediction]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source: 21th of August 2016\n",
            "output: 2000-02-20 \n",
            "\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "source: Tue 10 Jul 2007\n",
            "output: 2000-02-20 \n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "source: Saturday May 9 2018\n",
            "output: 2015-05-25 \n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "source: March 3 2001\n",
            "output: 2000-02-20 \n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "source: March 3rd 2001\n",
            "output: 2000-03-20 \n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "source: 1 March 2001\n",
            "output: 2000-02-20 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "s00 = np.zeros((1, n_s))\n",
        "c00 = np.zeros((1, n_s))\n",
        "for example in EXAMPLES:\n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    #print(source)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
        "    source = np.swapaxes(source, 0, 1)\n",
        "    source = np.expand_dims(source, axis=0)\n",
        "    prediction = model.predict([source, s00, c00])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output),\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "schema_names": [
        "DLSC5W3-1A"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}