{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ADgiCgrXtLy",
    "outputId": "2114357a-5ce5-4185-8ec4-d81e00681982",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id  sentiment                                             review\n",
      "0       5814_8          1  With all this stuff going down at the moment w...\n",
      "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "3       3630_4          0  It must be assumed that those who praised this...\n",
      "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
      "...        ...        ...                                                ...\n",
      "24995   3453_3          0  It seems like more consideration has gone into...\n",
      "24996   5064_1          0  I don't believe they made this film. Completel...\n",
      "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
      "24998  10194_3          0  This 30 minute documentary Bu√±uel made in the ...\n",
      "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
      "\n",
      "[25000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#opening the data file in read mode\n",
    "df = pd.read_csv('sentiment.tsv', sep='\\t')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ynk5CtWDbAfi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews=df.iloc[:,2].values\n",
    "labels=df.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "19GKV9hsnGt2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1343/920268986.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html, 'html.parser')\n"
     ]
    }
   ],
   "source": [
    "#parsing html\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def parseHtml(html):\n",
    "  soup = BeautifulSoup(html, 'html.parser')\n",
    "  return soup.get_text()\n",
    "\n",
    "def removeDigits(string):\n",
    "  for i in range(10):\n",
    "    string=string.replace(str(i),' ')\n",
    "  return string\n",
    "\n",
    "#removing html\n",
    "reviews=list(map(parseHtml, reviews))\n",
    "\n",
    "#removing digits\n",
    "reviews=list(map(removeDigits, reviews))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlJzPD7xROKl",
    "outputId": "f365341a-ed67-48e9-a0bc-763e3db34255",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "tokenizedText=[nltk.word_tokenize(item) for item in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dyjJY4RMRJd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#removing punctuation\n",
    "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "tokenizedText= [[word for word in review if word not in punc] for review in tokenizedText]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_list(nested_list):\n",
    "    flat_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flat_list.extend(flatten_list(item))\n",
    "        else:\n",
    "            flat_list.append(item)\n",
    "    return flat_list\n",
    "\n",
    "tokenizedText = flatten_list(tokenizedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5918433"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#splitting the Dataset into train and test set\n",
    "len(tokenizedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aCEDjcjMhmo",
    "outputId": "f8638505-93a0-44d9-b677-b384d3619017",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#splitting the Dataset into train and test set\n",
    "totalRows=np.shape(tokenizedText)[0]\n",
    "\n",
    "splitRatio=0.75\n",
    "splitPoint=int(splitRatio*totalRows)\n",
    "\n",
    "trainReviews=tokenizedText[:splitPoint]\n",
    "trainLabels=labels[:splitPoint]\n",
    "testReviews=tokenizedText[splitPoint:]\n",
    "testLabels=labels[splitPoint:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lejSbCrsBqXk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#learning word embeddings on training data using Gensim library\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "embeddingsSize=128\n",
    "model=Word2Vec(trainReviews, vector_size=embeddingsSize, window=5, min_count=1, workers=4)\n",
    "######################Training of Word Embeddings Vector Completed#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XakWbPClBg_z"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1343/4009779312.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrainReviewVectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainReviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtestReviewVectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestReviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1343/4009779312.py\u001b[0m in \u001b[0;36mgetVectors\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mwordCount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataItem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msingleDataItemEmbedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msingleDataItemEmbedding\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mwordCount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordCount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mvocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;34m\"The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;34m\"Use KeyedVector's .key_to_index dict, .index_to_key list, and methods \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\nUse KeyedVector's .key_to_index dict, .index_to_key list, and methods .get_vecattr(key, attr) and .set_vecattr(key, attr, new_val) instead.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def getVectors(dataset):\n",
    "  singleDataItemEmbedding=np.zeros(embeddingsSize)\n",
    "  vectors=[]\n",
    "  for dataItem in dataset:\n",
    "    wordCount=0\n",
    "    for word in dataItem:\n",
    "      if word in model.wv.vocab:\n",
    "        singleDataItemEmbedding=singleDataItemEmbedding+model.wv[word]\n",
    "        wordCount=wordCount+1\n",
    "\n",
    "    singleDataItemEmbedding=singleDataItemEmbedding/wordCount\n",
    "    vectors.append(singleDataItemEmbedding)\n",
    "  return vectors\n",
    "\n",
    "trainReviewVectors=getVectors(trainReviews)\n",
    "testReviewVectors=getVectors(testReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WhobrCpvDrUc"
   },
   "outputs": [],
   "source": [
    "#Let's define a function that can display the accuracy, F1-score, label-wise precision, recall, etc. of each classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#add path of google drive to environment variable to load python files from google drive\n",
    "import sys\n",
    "sys.path.insert(1, \"/content/drive/My Drive/nlp assignments/assignment 4\")\n",
    "from visualization import plot_confusion_matrix_from_data\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def printResults(y_true, y_predicted):\n",
    "  print(\"Accuracy= \", accuracy_score(y_true, y_predicted))\n",
    "\n",
    "  columns=['false', 'true']\n",
    "  plot_confusion_matrix_from_data(y_true, y_predicted, columns)\n",
    "\n",
    "  precision, recall, fscore, support = score(y_true, y_predicted)\n",
    "\n",
    "  print('###########################################')\n",
    "  print('precision: {}'.format(precision))\n",
    "  print('recall: {}'.format(recall))\n",
    "  print('fscore: {}'.format(fscore))\n",
    "  print('support: {}'.format(support))\n",
    "  print('###########################################3')\n",
    "\n",
    "  print('Macro F1 ',f1_score(y_true, y_predicted, average='macro'))\n",
    "\n",
    "  print('Micro F1 ', f1_score(y_true, y_predicted, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "x_nAxHEZCEtz",
    "outputId": "bd3c382d-4a45-43c5-f2fa-cc6c6207e287",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#naive bayes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clfNB = MultinomialNB()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaledTrainX= scaler.fit_transform(trainReviewVectors)\n",
    "scaledTestX = scaler.fit_transform(testReviewVectors)\n",
    "clfNB.fit(scaledTrainX, trainLabels)\n",
    "\n",
    "#test naive bayes accuracy\n",
    "testLabelsPredicted=list(clfNB.predict(scaledTestX))\n",
    "\n",
    "#print results\n",
    "print(\"####################RESULTS OF NAIVE BAYES CLASSIFIER##################\")\n",
    "printResults(testLabelsPredicted, testLabels)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
